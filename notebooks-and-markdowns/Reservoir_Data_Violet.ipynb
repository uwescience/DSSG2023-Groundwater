{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54413839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd2e801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lat and lon\n",
    "\n",
    "# Create a dictionary with key-value pairs\n",
    "upper_basin_reservoirs = {\n",
    "    \"100010\": ['Granby', 'Arizona', '40.15966','-105.86436' ],\n",
    "    \"919\": ['Lake Powell', 'Colorado', '36.93649', '-111.48396'],\n",
    "    \"100002\": ['Dillon', 'Colorado', '39.61614', '-106.05487'],\n",
    "    \"100089\": ['Green Mountain', 'Green Mountain','39.87964', '-106.30589'],\n",
    "    \"2002\": ['Ruedi', 'Colorado', '39.36564', '-106.8042'],\n",
    "    \"100053\": ['Williams Fork', 'Colorado','40.01882', '-106.21381'],\n",
    "    \"100049\": ['Shadow Mountain', 'Colorado', '40.22698', '-105.84385'],\n",
    "    \"917\": ['Flaming Gorge', 'Colorado', '40.91474', '-109.42185'],\n",
    "    \"962\": ['Strawberry', 'Utah', '40.13564', '-111.02659'],\n",
    "    \"916\": ['Fontenelle', 'Wyoming', '42.02617', '-110.06816'],\n",
    "    \"928\": ['Starvation', 'Utah', '42.02617', '-110.06816'],\n",
    "    \"931\": ['Scofield', 'Utah', '39.77656', '-111.05074'],\n",
    "    \"932\": ['Joes Valley', 'Utah', '39.2901', '-111.27888'],\n",
    "    \"930\": ['Moon Lake', 'Utah', '40.57445', '-110.50665'],\n",
    "    \"936\": ['Big Sandy', 'Wyoming', '42.24993', '-109.42803'],\n",
    "    \"963\": ['Upper Stillwater', 'Utah' ,'40.56565', '-110.70044'],\n",
    "    \"944\": ['Meeks Cabin', 'Utah', '41.01664', '-110.58344'],\n",
    "    \"949\": ['Stateline', 'Utah', '40.98291', '-110.39038'],\n",
    "    \"913\": ['Blue Mesa', 'Colorado', '38.45305', '-107.33677'],\n",
    "    \"914\": ['Morrow Point', 'Colorado', '38.45191', '-107.53791'],\n",
    "    \"912\": ['Taylor Park Reservoir', 'Colorado', '38.81844', '-106.60592'],\n",
    "    \"948\": ['Ridgway', 'Colorado', '38.23636', '-107.75914'],\n",
    "    \"915\": ['Crystal', 'Colorado', '38.51046', '-107.62374'],\n",
    "    \"958\": ['McPhee', 'Colorado', '37.57588', '-108.57307'],\n",
    "    \"920\": ['Navajo', \"New Mexico\", '36.80063', '-107.61203'],\n",
    "    \"933\": ['Vallecito', 'Colorado', '37.37834', '-107.57486'],\n",
    "    \"934\": ['Lemon', 'Colorado', '37.39538', '-107.57486'],\n",
    "    \n",
    "}\n",
    "\n",
    "lower_basin_reservoirs = {\n",
    "    \"921\": ['Lake Mead', 'AZ, NV', '36.0163', '-114.7374'],\n",
    "    \"922\": ['Lake Mohave', 'AZ, NV', '35.1979', '-114.5694'],\n",
    "    \"923\": ['Lake Havasu', 'AZ, CA', '34.2964', '-114.1385']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "crb = gpd.read_file(\"Colorado_River_Basin_Hydrological_Boundaries_with_Areas_served_by_Colorado_River.shp\")\n",
    "\n",
    "\n",
    "upper_basin_reservoirs_coordinates = [(float(lon), float(lat)) for _, (_, _, lat, lon) in upper_basin_reservoirs.items()]\n",
    "upper_basin_reservoirs_df = gpd.GeoDataFrame(geometry=gpd.points_from_xy([lon for lon, lat in upper_basin_reservoirs_coordinates], [lat for lon, lat in upper_basin_reservoirs_coordinates]))\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the Colorado River shapefile as the background\n",
    "crb.plot(ax=ax, color='blue')\n",
    "\n",
    "# Plot the reservoirs on top of the shapefile\n",
    "upper_basin_reservoirs_df.plot(ax=ax, color='red', marker='o', markersize=50)\n",
    "\n",
    "# Add title and labels if needed\n",
    "plt.title('Reservoirs along the Colorado River')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0f03d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for Granby...\n",
      "Downloading data for Lake Powell...\n",
      "Downloading data for Dillon...\n",
      "Downloading data for Green Mountain...\n",
      "Downloading data for Ruedi...\n",
      "Downloading data for Williams Fork...\n",
      "Downloading data for Shadow Mountain...\n",
      "Downloading data for Flaming Gorge...\n",
      "Downloading data for Strawberry...\n",
      "Downloading data for Fontenelle...\n",
      "Downloading data for Starvation...\n",
      "Downloading data for Scofield...\n",
      "Downloading data for Joes Valley...\n",
      "Downloading data for Moon Lake...\n",
      "Downloading data for Big Sandy...\n",
      "Downloading data for Upper Stillwater...\n",
      "Downloading data for Meeks Cabin...\n",
      "Downloading data for Stateline...\n",
      "Downloading data for Blue Mesa...\n",
      "Downloading data for Morrow Point...\n",
      "Downloading data for Taylor Park Reservoir...\n",
      "Downloading data for Ridgway...\n",
      "Downloading data for Crystal...\n",
      "Downloading data for McPhee...\n",
      "Downloading data for Navajo...\n",
      "Downloading data for Vallecito...\n",
      "Downloading data for Lemon...\n",
      "Downloading data for Lake Mead...\n",
      "Downloading data for Lake Mohave...\n",
      "Downloading data for Lake Havasu...\n",
      "        datetime   storage       lat         lon reservoir_name        basin\n",
      "0     1951-09-01  0.365136  40.15966  -105.86436         Granby  upper basin\n",
      "1     1951-09-02  0.365467  40.15966  -105.86436         Granby  upper basin\n",
      "2     1951-09-03  0.365667  40.15966  -105.86436         Granby  upper basin\n",
      "3     1951-09-04  0.365867  40.15966  -105.86436         Granby  upper basin\n",
      "4     1951-09-05  0.366065  40.15966  -105.86436         Granby  upper basin\n",
      "...          ...       ...       ...         ...            ...          ...\n",
      "30972 2023-07-19  0.709570   34.2964   -114.1385    Lake Havasu  lower basin\n",
      "30973 2023-07-20  0.713043   34.2964   -114.1385    Lake Havasu  lower basin\n",
      "30974 2023-07-21  0.711575   34.2964   -114.1385    Lake Havasu  lower basin\n",
      "30975 2023-07-22  0.709873   34.2964   -114.1385    Lake Havasu  lower basin\n",
      "30976 2023-07-23  0.710852   34.2964   -114.1385    Lake Havasu  lower basin\n",
      "\n",
      "[595374 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Function to download data from URL and handle retries\n",
    "def download_data(url):\n",
    "    max_attempts = 3\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            # Retrieve data from the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise exception for non-200 status codes\n",
    "\n",
    "            # Read the downloaded content into a DataFrame\n",
    "            df = pd.read_csv(io.StringIO(response.text))\n",
    "\n",
    "            # Convert the unit from acre-feet to cubic kilometers for the current reservoir\n",
    "            df['storage'] *= 0.000001233\n",
    "\n",
    "            # Extract relevant columns (datetime and storage)\n",
    "            df = df[['datetime', 'storage']]\n",
    "\n",
    "            # Add additional columns for reservoir name, latitude, and longitude\n",
    "            df['lat'] = info[2]\n",
    "            df['lon'] = info[3]\n",
    "            df['reservoir_name'] = info[0]\n",
    "           \n",
    "\n",
    "            return df  # Return the dataframe on successful download\n",
    "        except (requests.RequestException, pd.errors.ParserError) as e:\n",
    "            if attempt < max_attempts:\n",
    "                print(f\"Download attempt {attempt} failed. Retrying...\")\n",
    "                time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "            else:\n",
    "                print(\"Failed to download data. Max attempts reached.\")\n",
    "                return None\n",
    "\n",
    "template_url = \"https://www.usbr.gov/uc/water/hydrodata/reservoir_data/{}/csv/17.csv\"\n",
    "\n",
    "# Initialize an empty list to store dataframes for each reservoir\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the upper basin reservoirs\n",
    "for upper_reservoir_number, info in upper_basin_reservoirs.items():\n",
    "    # Generate the URL for the current reservoir\n",
    "    url = template_url.format(upper_reservoir_number)\n",
    "    print(f\"Downloading data for {info[0]}...\")\n",
    "    \n",
    "    # Download data for the current reservoir and add it to the list\n",
    "    df = download_data(url)\n",
    "    if df is not None:\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into a single dataframe for the upper basin\n",
    "upper_basin_df = pd.concat(dfs)\n",
    "\n",
    "# Reset the list of dataframes for the lower basin\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the lower basin reservoirs\n",
    "for lower_reservoir_number, info in lower_basin_reservoirs.items():\n",
    "    # Generate the URL for the current reservoir\n",
    "    url = template_url.format(lower_reservoir_number)\n",
    "    print(f\"Downloading data for {info[0]}...\")\n",
    "\n",
    "    # Download data for the current reservoir and add it to the list\n",
    "    df = download_data(url)\n",
    "    if df is not None:\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into a single dataframe for the lower basin\n",
    "lower_basin_df = pd.concat(dfs)\n",
    "\n",
    "# Convert 'datetime' column to datetime type for both dataframes\n",
    "upper_basin_df['datetime'] = pd.to_datetime(upper_basin_df['datetime'])\n",
    "upper_basin_df['basin'] = 'upper basin'\n",
    "lower_basin_df['datetime'] = pd.to_datetime(lower_basin_df['datetime'])\n",
    "lower_basin_df['basin'] = 'lower basin'\n",
    "storage_df = pd.concat([upper_basin_df, lower_basin_df], axis=0)\n",
    "# Display the combined dataframes\n",
    "print(storage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba264c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upper Basin: Wolford Mountain\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Retrieve the content from the URL\n",
    "url = \"https://waterdata.usgs.gov/co/nwis/dv?cb_00054=on&format=rdb&site_no=09041395&legacy=&referred_module=sw&period=&begin_date=1998-10-01&end_date=2023-07-18\"\n",
    "response = requests.get(url)\n",
    "file_content = response.text\n",
    "\n",
    "# Find the start position of the table\n",
    "start_pos = file_content.find(\"agency_cd\\tsite_no\\tdatetime\\t\")\n",
    "\n",
    "# Extract the table content\n",
    "table_content = file_content[start_pos:]\n",
    "\n",
    "# Convert the table content into a DataFrame\n",
    "Wolford_Mountain = pd.read_csv(StringIO(table_content), delimiter=\"\\t\")\n",
    "\n",
    "# Extract only the desired columns: 'datetime' and '18490_00054_32400' (Storage)\n",
    "Wolford_Mountain = Wolford_Mountain[['datetime', '18490_00054_32400']]\n",
    "\n",
    "# Rename the columns\n",
    "Wolford_Mountain.columns = ['datetime', 'storage']\n",
    "\n",
    "# Drop the first row\n",
    "Wolford_Mountain = Wolford_Mountain.drop(0)\n",
    "\n",
    "# Convert 'storage' column to numeric (ignore non-numeric values, set them to NaN)\n",
    "Wolford_Mountain['storage'] = pd.to_numeric(Wolford_Mountain['storage'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the 'storage' column\n",
    "Wolford_Mountain = Wolford_Mountain.dropna(subset=['storage'])\n",
    "\n",
    "# Add new columns for 'lat' and 'lon' with the provided values\n",
    "Wolford_Mountain['lat'] = '40.11294'\n",
    "Wolford_Mountain['lon'] = '-106.414'\n",
    "\n",
    "# Add a new column for 'reservoir' and set its value\n",
    "Wolford_Mountain['reservoir_name'] = 'Wolford Mountain'\n",
    "\n",
    "# Multiply 'storage' column by the conversion factor\n",
    "Wolford_Mountain['storage'] *= 0.000001233\n",
    "\n",
    "Wolford_Mountain['datetime'] = pd.to_datetime(Wolford_Mountain['datetime'])\n",
    "\n",
    "# Display the DataFrame with 'datetime', 'Storage', 'lat', 'lon', and 'reservoir' columns\n",
    "print(Wolford_Mountain)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5a4c1a1",
   "metadata": {},
   "source": [
    "#Lower Basin: \n",
    "\n",
    "\n",
    "\n",
    "# Retrieve the content from the URL\n",
    "url = \"https://waterdata.usgs.gov/nwis/dv?cb_00054=on&format=rdb&site_no=09509501&legacy=&referred_module=sw&period=&begin_date=1900-07-16&end_date=2023-07-16\"\n",
    "response = requests.get(url)\n",
    "file_content = response.text\n",
    "\n",
    "# Find the start position of the table\n",
    "start_pos = file_content.find(\"agency_cd\\tsite_no\\tdatetime\\t\")\n",
    "\n",
    "# Extract the table content\n",
    "table_content = file_content[start_pos:]\n",
    "\n",
    "# Convert the table content into a DataFrame\n",
    "Horseshoe = pd.read_csv(StringIO(table_content), delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "# Extract only the desired columns: 'datetime' and '18490_00054_32400' (Storage)\n",
    "Horseshoe = Horseshoe[['datetime', '5722_00054_32400']]\n",
    "\n",
    "# Rename the columns\n",
    "Horseshoe.columns = ['datetime', 'storage']\n",
    "\n",
    "# Drop the first row\n",
    "Horseshoe = Horseshoe.drop(0)\n",
    "\n",
    "# Convert 'storage' column to numeric (ignore non-numeric values, set them to NaN)\n",
    "Horseshoe['storage'] = pd.to_numeric(Horseshoe['storage'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the 'storage' column\n",
    "Horseshoe = Horseshoe.dropna(subset=['storage'])\n",
    "\n",
    "# Add new columns for 'lat' and 'lon' with the provided values\n",
    "Horseshoe['lat'] = '33.98476097'\n",
    "Horseshoe['lon'] = '-111.7104223'\n",
    "\n",
    "# Add a new column for 'reservoir' and set its value\n",
    "Horseshoe['reservoir_name'] = 'Horseshoe'\n",
    "\n",
    "# Multiply 'storage' column by the conversion factor\n",
    "Horseshoe['storage'] *= 0.000001233\n",
    "\n",
    "\n",
    "print(Horseshoe)\n",
    "\n",
    "\n",
    "#Same needs to be done with these urls: #San Carlos: https://nwis.waterservices.usgs.gov/nwis/iv/?sites=09469000&parameterCd=00054&startDT=2007-10-01T00:00:00.000-07:00&endDT=2023-07-24T23:59:59.999-07:00&siteStatus=all&format=rdb\n",
    "#https://waterdata.usgs.gov/nwis/dv?cb_00054=on&format=rdb&site_no=09509502&legacy=&referred_module=sw&period=&begin_date=1964-07-13&end_date=2023-07-19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
